@article{DeOliveiraNeto2019,
  doi = {10.1016/j.jss.2019.07.002},
  url = {https://doi.org/10.1016/j.jss.2019.07.002},
  year = {2019},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {156},
  pages = {246--267},
  author = {Francisco Gomes {de Oliveira Neto} and Richard Torkar and Robert Feldt and Lucas Gren and Carlo A. Furia and Ziwei Huang},
  title = {Evolution of statistical analysis in empirical software engineering research: Current state and steps forward},
  journal = jss,
  abstract = {Software engineering research is evolving and papers are increasingly based on empirical data from a multitude of sources, using statistical tests to determine if and to what degree empirical evidence supports their hypotheses. To investigate the practices and trends of statistical analysis in empirical software engineering (ESE), this paper presents a review of a large pool of papers from top-ranked software engineering journals. First, we manually reviewed 161 papers and in the second phase of our method, we conducted a more extensive semi-automatic classification of papers spanning the years 2001--2015 and 5,196 papers. Results from both review steps was used to: i) identify and analyze the predominant practices in ESE (e.g., using t-test or ANOVA), as well as relevant trends in usage of specific statistical methods (e.g., nonparametric tests and effect size measures) and, ii) develop a conceptual model for a statistical analysis workflow with suggestions on how to apply different statistical methods as well as guidelines to avoid pitfalls. Lastly, we confirm existing claims that current ESE practices lack a standard to report practical significance of results. We illustrate how practical significance can be discussed in terms of both the statistical analysis and in the practitioner's context.}
}

@inproceedings{Devanbu2016,
  doi = {10.1145/2884781.2884812},
  url = {https://doi.org/10.1145/2884781.2884812},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Prem Devanbu and Thomas Zimmermann and Christian Bird},
  title = {Belief {\&} evidence in empirical software engineering},
  booktitle = icse,
  abstract = {Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highly trained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.}
}

@article{Dyba2006,
  doi = {10.1016/j.infsof.2005.08.009},
  url = {https://doi.org/10.1016/j.infsof.2005.08.009},
  year = {2006},
  month = aug,
  publisher = {Elsevier {BV}},
  volume = {48},
  number = {8},
  pages = {745--755},
  author = {Tore Dyb{\aa} and Vigdis By Kampenes and Dag I.K. Sj{\o}berg},
  title = {A systematic review of statistical power in software engineering experiments},
  journal = ist,
  abstract = {Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993--2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study's findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.}
}

@inproceedings{Flint2021,
  doi = {10.1109/msr52588.2021.00022},
  url = {https://doi.org/10.1109/msr52588.2021.00022},
  year = {2021},
  month = may,
  publisher = {{IEEE}},
  author = {Samuel W. Flint and Jigyasa Chauhan and Robert Dyer},
  title = {Escaping the Time Pit: Pitfalls and Guidelines for Using Time-Based Git Data},
  booktitle = msr,
  abstract = {Many software engineering research papers rely on time-based data (e.g., commit timestamps, issue report creation/update/close dates, release dates). Like most real-world data however, time-based data is often dirty. To date, there are no studies that quantify how frequently such data is used by the software engineering research community, or investigate sources of and quantify how often such data is dirty. Depending on the research task and method used, including such dirty data could affect the research results. This paper presents the first survey of papers that utilize time-based data, published in the Mining Software Repositories (MSR) conference series. Out of the 690 technical track and data papers published in MSR 2004--2020, we saw at least 35\% of papers utilized time-based data. We then used the Boa and Software Heritage infrastructures to help identify and quantify several sources of dirty commit timestamp data. Finally we provide guidelines/best practices for researchers utilizing time-based data from Git repositories.}
}

@article{Furia2019,
  doi = {10.1109/tse.2019.2935974},
  url = {https://doi.org/10.1109/tse.2019.2935974},
  year = {2019},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pages = {1--1},
  author = {Carlo Alberto Furia and Robert Feldt and Richard Torkar},
  title = {Bayesian Data Analysis in Empirical Software Engineering Research},
  journal = ieee-tse,
  abstract = {Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics have traditionally dominated empirical data analysis, and certainly remain prevalent in empirical software engineering. This situation is unfortunate because frequentist statistics suffer from a number of shortcomings---such as lack of flexibility and results that are unintuitive and hard to interpret---that curtail their effectiveness when dealing with the heterogeneous data that is increasingly available for empirical analysis of software engineering practice. In this paper, we pinpoint these shortcomings, and present Bayesian data analysis techniques that work better on the same data---as they can provide clearer results that are simultaneously robust and nuanced. After a short, high-level introduction to the basic tools of Bayesian statistics, our presentation targets the reanalysis of two empirical studies targeting data about the effectiveness of automatically generated tests and the performance of programming languages. By contrasting the original frequentist analysis to our new Bayesian analysis, we demonstrate concrete advantages of using Bayesian techniques, and we advocate a prominent role for them in empirical software engineering research and practice.}
}

@article{Ma2021,
  doi = {10.1007/s10664-020-09905-9},
  url = {https://doi.org/10.1007/s10664-020-09905-9},
  year = {2021},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {26},
  number = {2},
  author = {Yuxing Ma and Tapajit Dey and Chris Bogart and Sadika Amreen and Marat Valiev and Adam Tutko and David Kennard and Russell Zaretzki and Audris Mockus},
  title = {World of code: enabling a research workflow for mining and analyzing the universe of open source {VCS} data},
  journal = ese,
  abstract = {Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are the tens of millions of projects in the periphery interconnected through. technical dependencies, code sharing, or knowledge flow? To answer such questions we: a) create a very large and frequently updated collection of version control data in the entire FLOSS ecosystems named World of Code (WoC), that can completely cross-reference authors, projects, commits, blobs, dependencies, and history of the FLOSS ecosystems and b) provide capabilities to efficiently correct, augment, query, and analyze that data. Our current WoC implementation is capable of being updated on a monthly basis and contains over 18B Git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.}
}

@article{Pizard2021,
  doi = {10.1007/s10664-021-09953-9},
  url = {https://doi.org/10.1007/s10664-021-09953-9},
  year = {2021},
  month = mar,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {26},
  number = {3},
  author = {Sebasti\'{a}n Pizard and Fernando Acerenza and Ximena Otegui and Silvana Moreno and Diego Vallespir and Barbara Kitchenham},
  title = {Training students in evidence-based software engineering and systematic reviews: a systematic review and empirical study},
  journal = ese,
  abstract = {Context Although influential in academia, evidence-based software engineering (EBSE) has had little impact on industry practice. We found that other disciplines have identified lack of training as a significant barrier to Evidence-Based Practice. Objective To build and assess an EBSE training proposal suitable for students with more than 3 years of computer science/software engineering university-level training. Method We performed a systematic literature review (SLR) of EBSE teaching initiatives and used the SLR results to help us to develop and evaluate an EBSE training proposal. The course was based on the theory of learning outcomes and incorporated a large practical content related to performing an SLR. We ran the course with 10 students and based course evaluation on student performance and opinions of both students and teachers. We assessed knowledge of EBSE principles from the mid-term and final tests, as well as evaluating the SLRs produced by the student teams. We solicited student opinions about the course and its value via a student survey, a team survey, and a focus group. The teachers' viewpoint was collected in a debriefing meeting. Results Our SLR identified 14 relevant primary studies. The primary studies emphasized the importance of practical examples (usually based on the SLR process) and used a variety of evaluation methods, but lacked any formal education methodology. We identified 54 learning outcomes covering aspects of EBSE and the SLR method. All 10 students passed the course. Our course evaluation showed that a large percentage of the learning outcomes established for training were accomplished. Conclusions The course proved suitable for students to understand the EBSE paradigm and to be able to apply it to a limited-scope practical assignment. Our learning outcomes, course structure, and course evaluation process should help to improve the effectiveness and comparability of future studies of EBSE training. However, future courses should increase EBSE training related to the use of SLR results.}
}

@inproceedings{Reyes2018,
  doi = {10.1145/3180155.3180161},
  url = {https://doi.org/10.1145/3180155.3180161},
  year = {2018},
  month = may,
  publisher = {{ACM}},
  author = {Rolando P. Reyes and Oscar Dieste and Efra\'{\i}n R. Fonseca and Natalia Juristo},
  title = {Statistical errors in software engineering experiments},
  booktitle = icse,
  abstract = {Background: Statistical concepts and techniques are often applied incorrectly, even in mature disciplines such as medicine or psychology. Surprisingly, there are very few works that study statistical problems in software engineering (SE). Aim: Assess the existence of statistical errors in SE experiments. Method: Compile the most common statistical errors in experimental disciplines. Survey experiments published in ICSE to assess whether errors occur in high quality SE publications. Results: The same errors as identified in others disciplines were found in ICSE experiments, where 30 of the reviewed papers included several error types such as: a) missing statistical hypotheses, b) missing sample size calculation, c) failure to assess statistical test assumptions, and d) uncorrected multiple testing. This rather large error rate is greater for research papers where experiments are confined to the validation section. The origin of the errors can be traced back to: a) researchers not having sufficient statistical training, and b) a profusion of exploratory research. Conclusions: This paper provides preliminary evidence that SE research suffers from the same statistical problems as other experimental disciplines. However, the SE community appears to be unaware of any shortcomings in its experiments, whereas other disciplines work hard to avoid these threats. Further research is necessary to find the underlying causes and set up corrective measures, but there are some potentially effective actions and are a priori easy to implement: a) improve the statistical training of SE researchers, and b) enforce quality assessment and reporting guidelines in SE publications.}
}

@article{Stol2018,
  doi = {10.1145/3241743},
  url = {https://doi.org/10.1145/3241743},
  year = {2018},
  month = oct,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {27},
  number = {3},
  pages = {1--51},
  author = {Klaas-Jan Stol and Brian Fitzgerald},
  title = {The {ABC} of Software Engineering Research},
  journal = acm-tosem,
  abstract = {A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.}
}

@article{Graziotin2022,
  doi = {10.1145/3469888},
  url = {https://doi.org/10.1145/3469888},
  year = {2022},
  month = jan,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {31},
  number = {1},
  pages = {1--36},
  author = {Daniel Graziotin and Per Lenberg and Robert Feldt and Stefan Wagner},
  title = {Psychometrics in Behavioral Software Engineering: A Methodological Introduction with Guidelines},
  journal = {{ACM} Transactions on Software Engineering and Methodology}
}

@article{Prenner2022,
  doi = {10.1109/tse.2021.3135465},
  url = {https://doi.org/10.1109/tse.2021.3135465},
  year = {2022},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pages = {1--1},
  author = {Julian Aron Aron Prenner and Romain Robbes},
  title = {Making the most of small Software Engineering datasets with modern machine learning},
  journal = {{IEEE} Transactions on Software Engineering}
}

